{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HOF-pretrained-classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "FWeac-WOhA0V"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sentence --->Cleaning--> Normalization --> Tokenizer ---> Vocab2number ---> Padding ---> Numerical input --->\n",
        "#               Word2vecModel---> Embedding(sentences, words, features) ---> LSTM (sentences, words, features) ---> Dense ---> Prediction\n",
        "#                 ||                      ||                                                ||                        ||\n",
        "#             (setnences, features)     (setnences, words, features)                  (setnences, features)         (setnences, labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n"
      ],
      "metadata": {
        "id": "JHZvrrwQSdM0"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('HoF_data.tsv', sep='\\t')\n",
        "X, Y = df.Tweet, df['Task A ']\n",
        "\n"
      ],
      "metadata": {
        "id": "zGpb9TxeS8XP"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def text_transformation(text_data):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(text_data)\n",
        "    transformed_text = tokenizer.texts_to_sequences(text_data)\n",
        "    padded_text = pad_sequences(transformed_text, padding=\"pre\", value=0)\n",
        "    return tokenizer, padded_text\n"
      ],
      "metadata": {
        "id": "mEjy9CU0TGSo"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inverse_transformation(tokenizer, test_data):\n",
        "    tokenizer_dict = dict((v, k) for k,v in tokenizer.word_index.items())\n",
        "    sentences = []\n",
        "    for transformed_text in tokenizer.texts_to_sequences(test_data):\n",
        "        sentence = []\n",
        "        for token in transformed_text:\n",
        "            sentence.append(tokenizer_dict[token])\n",
        "        sentences.append(sentence)\n",
        "        del sentence\n",
        "    return sentences\n"
      ],
      "metadata": {
        "id": "-Ipjnc_AhH6o"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrained_embedding_creation(tokenizer, text_data, vector_size, model_name):\n",
        "    tokenized_text = inverse_transformation(tokenizer, text_data)\n",
        "    word2vec_model = Word2Vec(tokenized_text, size=vector_size, window=3, min_count=1)\n",
        "    word2vec_model.save(model_name)\n"
      ],
      "metadata": {
        "id": "jxHLmvT4Wp0Z"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encoder(Y):\n",
        "    class_label_encoder = LabelEncoder()\n",
        "    class_label_encoder.fit(Y)\n",
        "    y_transformed = class_label_encoder.transform(Y).reshape(-1, 1)\n",
        "    return class_label_encoder, y_transformed\n",
        "\n"
      ],
      "metadata": {
        "id": "Tn_OeryVWWpm"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def map_vector(tokenizer, word2vec_model, vector_dim):\n",
        "    vocab = tokenizer.word_index\n",
        "    vocab_size = len(vocab) + 1\n",
        "    weight_matrix = np.zeros((vocab_size, vector_dim))\n",
        "\n",
        "    for word, i in vocab.items():\n",
        "        try:\n",
        "            weight_matrix[i] = word2vec_model.wv[word]\n",
        "        except:\n",
        "            pass\n",
        "    return weight_matrix\n"
      ],
      "metadata": {
        "id": "btrOM2H3hf8-"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter and Hyper-parameter initialization\n",
        "embedding_dim = [25, 50, 100, 200]\n",
        "lstm_units = 50\n",
        "batch_size = 10\n",
        "epoches = 10\n",
        "test_data = 0.2\n",
        "\n",
        "Tokenizer_obj, x_data = text_transformation(X)\n",
        "label_encoder, y_data = label_encoder(Y)\n",
        "pretrained_embedding_creation(Tokenizer_obj, X, embedding_dim[1], 'HoF_embedding.50d.txt')\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x_data, y_data, test_size=test_data, shuffle=True)\n"
      ],
      "metadata": {
        "id": "YyPJwGnikX3N"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained embedding from word2vec model\n",
        "w2v_model = Word2Vec.load('HoF_embedding.50d.txt')\n",
        "print(w2v_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMpX6GQdivzs",
        "outputId": "6cfbb985-ffac-48a9-a25a-8f121559423c"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec(vocab=15497, size=50, alpha=0.025)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For work with Glove word embedding\n",
        "# steps:\n",
        "# 1. Download Glove pretrained embedding. Here we are using Twitter pretrained embedding which contains 25, 50, 100, 200 dimentional vectors. We'll use all dimentional vectors and compare the results.\n",
        "# 2. Use Gensim library to convert Glove vector to Word2vec vector\n",
        "# 3. Load Wordvec model with the Word2Vec.load function of gensim library\n",
        "\n",
        "# !wget http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
        "# !unzip glove*.zip"
      ],
      "metadata": {
        "id": "U0idoOK-SKKT"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map numerical representation with pretrained embedding with respect to the token\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "# glove2word2vec(glove_input_file=\"glove.twitter.27B.25d.txt\", word2vec_output_file=\"word2vec_glove_vectors.25d.txt\")\n",
        "glove_model_25d = KeyedVectors.load_word2vec_format(\"word2vec_glove_vectors.25d.txt\", binary=False)\n",
        "glove_25_weight_matrix = map_vector(Tokenizer_obj, glove_model_25d, embedding_dim[0])\n",
        "\n",
        "# glove2word2vec(glove_input_file=\"glove.twitter.27B.50d.txt\", word2vec_output_file=\"word2vec_glove_vectors.50d.txt\")\n",
        "glove_model_50d = KeyedVectors.load_word2vec_format(\"word2vec_glove_vectors.50d.txt\", binary=False)\n",
        "glove_50_weight_matrix = map_vector(Tokenizer_obj, glove_model_50d, embedding_dim[1])\n",
        "\n",
        "# glove2word2vec(glove_input_file=\"glove.twitter.27B.100d.txt\", word2vec_output_file=\"word2vec_glove_vectors.100d.txt\")\n",
        "glove_model_100d = KeyedVectors.load_word2vec_format(\"word2vec_glove_vectors.100d.txt\", binary=False)\n",
        "glove_100_weight_matrix = map_vector(Tokenizer_obj, glove_model_100d, embedding_dim[2])\n",
        "\n",
        "# glove2word2vec(glove_input_file=\"glove.twitter.27B.200d.txt\", word2vec_output_file=\"word2vec_glove_vectors.200d.txt\")\n",
        "glove_model_200d = KeyedVectors.load_word2vec_format(\"word2vec_glove_vectors.200d.txt\", binary=False)\n",
        "glove_200_weight_matrix = map_vector(Tokenizer_obj, glove_model_200d, embedding_dim[3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gp4IWDgT3ey",
        "outputId": "38aa71fb-e11a-48a4-9410-f6fcfe7cf6df"
      },
      "execution_count": 134,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "local_weight_matrix = map_vector(Tokenizer_obj, w2v_model, embedding_dim[1])"
      ],
      "metadata": {
        "id": "WPexN9J0gbRe"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "vocab_size = 1 x 10000\n",
        "output = 1 x 100\n",
        "1.  Pretrained embeddings (PE)\n",
        "2.  Input(Embedding Layer) == Output(Pretrained Embedding)\n",
        "tokenizer.word_index = {'the':1, 'is':2, 'us':3, ......., 'home':10000}\n",
        "PE = {'is':1, 'home':2, 'us':3, ......, 'the':10010}\n",
        "'''\n",
        "\n",
        "\n",
        "def singleLayerModel(vocab_size, embedding_dim, lstm_units, pretrained_embedding):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[pretrained_embedding], trainable=False, mask_zero=True))\n",
        "    model.add(LSTM(units=lstm_units, return_sequences=False))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    print(model.summary())\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "GP--TOkPhj3A"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sDLM_25 = singleLayerModel(len(Tokenizer_obj.word_index)+1, embedding_dim[0], lstm_units, glove_25_weight_matrix)\n",
        "sDLM_25_history = sDLM_25.fit(X_train, Y_train, epochs=epoches, batch_size=batch_size, validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Mn_TTTIYsTt",
        "outputId": "b741abdc-15ee-4e03-930f-0490dc6fbe31"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_12 (Embedding)    (None, None, 25)          387450    \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 50)                15200     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 402,701\n",
            "Trainable params: 15,251\n",
            "Non-trainable params: 387,450\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "282/282 [==============================] - 92s 310ms/step - loss: 0.6886 - accuracy: 0.5420 - val_loss: 0.6783 - val_accuracy: 0.5554\n",
            "Epoch 2/10\n",
            "282/282 [==============================] - 85s 303ms/step - loss: 0.6732 - accuracy: 0.5679 - val_loss: 0.6593 - val_accuracy: 0.6108\n",
            "Epoch 3/10\n",
            "282/282 [==============================] - 85s 302ms/step - loss: 0.6675 - accuracy: 0.5868 - val_loss: 0.6583 - val_accuracy: 0.6278\n",
            "Epoch 4/10\n",
            "282/282 [==============================] - 87s 308ms/step - loss: 0.6570 - accuracy: 0.6014 - val_loss: 0.6547 - val_accuracy: 0.6236\n",
            "Epoch 5/10\n",
            "282/282 [==============================] - 86s 304ms/step - loss: 0.6500 - accuracy: 0.6170 - val_loss: 0.6769 - val_accuracy: 0.5952\n",
            "Epoch 6/10\n",
            "282/282 [==============================] - 86s 305ms/step - loss: 0.6424 - accuracy: 0.6287 - val_loss: 0.6467 - val_accuracy: 0.6165\n",
            "Epoch 7/10\n",
            "282/282 [==============================] - 86s 305ms/step - loss: 0.6299 - accuracy: 0.6437 - val_loss: 0.6460 - val_accuracy: 0.6165\n",
            "Epoch 8/10\n",
            "282/282 [==============================] - 87s 307ms/step - loss: 0.6171 - accuracy: 0.6440 - val_loss: 0.6494 - val_accuracy: 0.6207\n",
            "Epoch 9/10\n",
            "282/282 [==============================] - 87s 308ms/step - loss: 0.6039 - accuracy: 0.6622 - val_loss: 0.6564 - val_accuracy: 0.6122\n",
            "Epoch 10/10\n",
            "282/282 [==============================] - 85s 300ms/step - loss: 0.5843 - accuracy: 0.6767 - val_loss: 0.6891 - val_accuracy: 0.6080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sDLM_50 = singleLayerModel(len(Tokenizer_obj.word_index)+1, embedding_dim[1], lstm_units, glove_50_weight_matrix)\n",
        "sDLM_50_history = sDLM_50.fit(X_train, Y_train, epochs=epoches, batch_size=batch_size, validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "id": "aqQxxxSxYv2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sDLM_100 = singleLayerModel(len(Tokenizer_obj.word_index)+1, embedding_dim[2], lstm_units, glove_100_weight_matrix)\n",
        "sDLM_100_history = sDLM_100.fit(X_train, Y_train, epochs=epoches, batch_size=batch_size, validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "id": "thTuO6zGYwV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sDLM_200 = singleLayerModel(len(Tokenizer_obj.word_index)+1, embedding_dim[3], lstm_units, glove_200_weight_matrix)\n",
        "sDLM_200_history = sDLM_200.fit(X_train, Y_train, epochs=epoches, batch_size=batch_size, validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "id": "ZARFtJOHYwwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DLM_local = singleLayerModel(len(Tokenizer_obj.word_index)+1, embedding_dim[1], lstm_units, local_weight_matrix)\n",
        "sDLM_local_history = DLM_local.fit(X_train, Y_train, epochs=epoches, batch_size=batch_size, validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "id": "b_qlVLr4Y6eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = [sDLM_local_history, sDLM_25_history, sDLM_50_history, sDLM_100_history, sDLM_200_history]\n",
        "history_name = ['sDLM_local_history', 'sDLM_25_history', 'sDLM_50_history', 'sDLM_100_history', 'sDLM_200_history']\n",
        "\n",
        "his_train_acc, his_val_acc, his_train_loss, his_val_loss = dict(), dict(), dict(), dict()\n",
        "\n",
        "for idx, hist in enumerate(history):\n",
        "  his_train_acc[history_name[idx]] = hist.history['accuracy']\n",
        "  his_val_acc[history_name[idx]] = hist.history['val_accuracy']\n",
        "\n",
        "  his_train_loss[history_name[idx]] = hist.history['loss']\n",
        "  his_val_loss[history_name[idx]] = hist.history['val_loss']"
      ],
      "metadata": {
        "id": "7LikEQm0aUqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.DataFrame(his_train_acc))"
      ],
      "metadata": {
        "id": "jZKA_C_vmjv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.DataFrame(his_train_loss))"
      ],
      "metadata": {
        "id": "omDXKfi6mpXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.DataFrame(his_val_acc))"
      ],
      "metadata": {
        "id": "8kYMocITmp-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.DataFrame(his_val_loss))"
      ],
      "metadata": {
        "id": "q11gebqRmqQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The prediction is shown on a single trained model, sDLM_local_history. By the same way, you can use the other trained models to predict and perform evaluations.\n",
        "predictedion = sDLM_local_history.predict(X_test)"
      ],
      "metadata": {
        "id": "PSMbIjTnhmXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_class = list()\n",
        "for pred_prob in predictedion:\n",
        "  if pred_prob >= 0.5:\n",
        "    predict_class.append(1)\n",
        "  else:\n",
        "    predict_class.append(0)\n",
        "\n",
        "predicted_labels = label_encoder.inverse_transform(predict_class)\n",
        "Y_test_labels = label_encoder.inverse_transform(Y_test)"
      ],
      "metadata": {
        "id": "NEWvYTdJhovJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "score = metrics.classification_report(Y_test_labels, predicted_labels, target_names=label_encoder.classes_)\n",
        "cf_matrix = metrics.confusion_matrix(Y_test_labels, predicted_labels, labels=label_encoder.classes_)\n",
        "print(score)\n",
        "print(cf_matrix)\n"
      ],
      "metadata": {
        "id": "uDbDyC28hqo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
        "group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
        "group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "ax= plt.subplot()\n",
        "sn.heatmap(cf_matrix, annot=labels, fmt='') # font size\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.xaxis.set_ticklabels(label_encoder.classes_)\n",
        "ax.yaxis.set_ticklabels(label_encoder.classes_)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VhMeittulmQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "viIg9idXR7__"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}